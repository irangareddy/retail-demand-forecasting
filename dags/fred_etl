"""DAG for FRED data ingestion with proper rules file handling."""

# version: 1.0.0

from airflow import DAG
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.decorators import task
from airflow.models import Variable
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path
import shutil

from retail_data_sources.fred.fred_api_handler import FREDAPIHandler
from retail_data_sources.fred.models.economic_metrics import EconomicData
from retail_data_sources.snowflake.fred import FredSnowflake
from retail_data_sources.utils.logging import setup_logger

# Constants
FRED_BASE_DIR = "/opt/airflow/data/fred"
RULES_FILE = Path(__file__).parents[2] / "retail_data_sources" / "fred" / "fred_interpretation_rules.json"
logger = setup_logger()

@task
def setup_directories():
    """Create necessary directories and copy rules file."""
    try:
        # Create main data directory
        data_dir = Path(FRED_BASE_DIR)
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # Create tmp subdirectory for FRED data
        tmp_dir = data_dir / "tmp"
        tmp_dir.mkdir(exist_ok=True)

        # Copy rules file from package to FRED directory
        if RULES_FILE.exists():
            dest_rules = data_dir / "fred_interpretation_rules.json"
            shutil.copy(RULES_FILE, dest_rules)
            logger.info(f"Copied rules file to {dest_rules}")
        else:
            raise FileNotFoundError(f"Rules file not found at {RULES_FILE}")
        
        logger.info(f"Created directory structure at {FRED_BASE_DIR}")
    except Exception as e:
        logger.error(f"Error in setup: {e}")
        raise

@task
def fetch_fred_data():
    """Fetch economic metrics from FRED API."""
    logger.info("Starting to fetch FRED economic data...")
    try:
        api_key = Variable.get("FRED_API_KEY")
        rules_file = Path(FRED_BASE_DIR) / "fred_interpretation_rules.json"
        
        handler = FREDAPIHandler(
            api_key=api_key,
            base_dir=FRED_BASE_DIR,
            rules_file=str(rules_file)
        )
        economic_data = handler.process_data(fetch=True)
        if not economic_data:
            raise ValueError("No data returned from FRED API")
            
        logger.info("Successfully fetched FRED data")
        return economic_data.to_dict()
    except Exception as e:
        logger.error(f"Error fetching FRED data: {e}")
        raise

@task
def save_raw_to_snowflake(data: dict):
    """Save raw FRED data to Snowflake using FredSnowflake class."""
    try:
        hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
        conn = hook.get_conn()
        
        fred_snowflake = FredSnowflake()
        target_schema = Variable.get("SNOWFLAKE_TARGET_SCHEMA", "RAW")
        
        # Prepare SQL statements
        create_sql, records, merge_sql = fred_snowflake.prepare_load_sql(
            economic_data=data,
            target_schema=target_schema,
            target_table='FRED_ECONOMIC_DATA'
        )
        
        cur = conn.cursor()
        try:
            # Execute create statement
            cur.execute(create_sql)
            
            # Execute merge for each record
            for record in records:
                cur.execute(merge_sql, record)
            
            conn.commit()
            logging.info("Successfully saved raw FRED data to Snowflake")
            
        finally:
            cur.close()
            conn.close()
            
    except Exception as e:
        logging.error(f"Error saving raw data to Snowflake: {e}")
        raise

@task
def cleanup_directories():
    """Clean up temporary FRED data files."""
    try:
        tmp_dir = Path(FRED_BASE_DIR) / "tmp"
        if tmp_dir.exists():
            # import shutil
            # shutil.rmtree(tmp_dir)
            logging.info(f"Cleaned up temporary directory: {tmp_dir}")
    except Exception as e:
        logging.error(f"Error cleaning up directories: {e}")
        # Don't raise as this is cleanup

with DAG(
    'fred_to_snowflake_dbt',
    default_args={
        'owner': 'airflow',
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    },
    description='FRED data pipeline with dbt transformations',
    schedule_interval='0 1 * * *',  # Run daily at 1 AM
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    # Define task dependencies
    setup = setup_directories()
    fred_data = fetch_fred_data()
    raw_save = save_raw_to_snowflake(fred_data)
    cleanup = cleanup_directories()
    
    # Set up complete pipeline
    setup >> fred_data >> raw_save >> cleanup